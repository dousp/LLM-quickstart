{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers 模型量化技术：AWQ（OPT-2.7B）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "- 资源有限，上传也是噩梦，这里改用`opt-125m`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ",
    "tags": []
   },
   "source": [
    "## 使用 AutoAWQ 量化模型\n",
    "\n",
    "下面我们以 `facebook opt-2.7B` 模型为例，使用 `AutoAWQ` 库实现的 AWQ 算法实现模型量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:04:10.036872Z",
     "iopub.status.busy": "2024-04-08T11:04:10.036488Z",
     "iopub.status.idle": "2024-04-08T11:04:10.074416Z",
     "shell.execute_reply": "2024-04-08T11:04:10.073825Z",
     "shell.execute_reply.started": "2024-04-08T11:04:10.036843Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"/mnt/workspace/dataset/mit-han-lab/pile-val-backup\"\n",
    "datasets = load_dataset(path=\"/mnt/workspace/dataset/mit-han-lab/pile-val-backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T08:48:26.707408Z",
     "iopub.status.busy": "2024-04-08T08:48:26.707036Z",
     "iopub.status.idle": "2024-04-08T08:48:26.711714Z",
     "shell.execute_reply": "2024-04-08T08:48:26.710973Z",
     "shell.execute_reply.started": "2024-04-08T08:48:26.707384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'meta'],\n",
       "        num_rows: 214670\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T09:11:34.870113Z",
     "iopub.status.busy": "2024-04-08T09:11:34.869383Z",
     "iopub.status.idle": "2024-04-08T09:11:34.873373Z",
     "shell.execute_reply": "2024-04-08T09:11:34.872773Z",
     "shell.execute_reply.started": "2024-04-08T09:11:34.870078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"/mnt/workspace/models/facebook/opt-2.7b\"\n",
    "quant_model_dir =    \"/mnt/workspace/models/facebook/opt-2.7b-autoawq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T09:13:55.483246Z",
     "iopub.status.busy": "2024-04-08T09:13:55.482879Z",
     "iopub.status.idle": "2024-04-08T09:13:55.486387Z",
     "shell.execute_reply": "2024-04-08T09:13:55.485738Z",
     "shell.execute_reply.started": "2024-04-08T09:13:55.483222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:05:30.239137Z",
     "iopub.status.busy": "2024-04-08T11:05:30.238771Z",
     "iopub.status.idle": "2024-04-08T11:05:31.322726Z",
     "shell.execute_reply": "2024-04-08T11:05:31.322047Z",
     "shell.execute_reply.started": "2024-04-08T11:05:30.239112Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True,data_files=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:26:47.092510Z",
     "iopub.status.busy": "2024-04-08T11:26:47.092128Z",
     "iopub.status.idle": "2024-04-08T11:36:20.557358Z",
     "shell.execute_reply": "2024-04-08T11:36:20.556801Z",
     "shell.execute_reply.started": "2024-04-08T11:26:47.092483Z"
    },
    "id": "Qn_P_E5p7gAN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 32/32 [09:26<00:00, 17.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config,calib_data=dataset_path,split = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T11:49:39.062293Z",
     "iopub.status.busy": "2024-04-08T11:49:39.061934Z",
     "iopub.status.idle": "2024-04-08T11:49:41.075059Z",
     "shell.execute_reply": "2024-04-08T11:49:41.074531Z",
     "shell.execute_reply.started": "2024-04-08T11:49:39.062268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/workspace/models/facebook/opt-2.7b-autoawq/tokenizer_config.json',\n",
       " '/mnt/workspace/models/facebook/opt-2.7b-autoawq/special_tokens_map.json',\n",
       " '/mnt/workspace/models/facebook/opt-2.7b-autoawq/vocab.json',\n",
       " '/mnt/workspace/models/facebook/opt-2.7b-autoawq/merges.txt',\n",
       " '/mnt/workspace/models/facebook/opt-2.7b-autoawq/added_tokens.json',\n",
       " '/mnt/workspace/models/facebook/opt-2.7b-autoawq/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "model.model.config.quantization_config = quantization_config\n",
    "\n",
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN",
    "tags": []
   },
   "source": [
    "#### Transformers 兼容性配置\n",
    "\n",
    "为了使`quant_config` 与 transformers 兼容，我们需要修改配置文件：`使用 Transformers.AwqConfig 来实例化量化模型配置`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/opt-2.7b-awq/tokenizer_config.json',\n",
       " 'models/opt-2.7b-awq/special_tokens_map.json',\n",
       " 'models/opt-2.7b-awq/vocab.json',\n",
       " 'models/opt-2.7b-awq/merges.txt',\n",
       " 'models/opt-2.7b-awq/added_tokens.json',\n",
       " 'models/opt-2.7b-awq/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptAWQForCausalLM(\n",
       "  (model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=True, w_bit=4, group_size=128)\n",
       "              (v_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=True, w_bit=4, group_size=128)\n",
       "              (q_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=True, w_bit=4, group_size=128)\n",
       "              (out_proj): WQLinear_GEMM(in_features=2560, out_features=2560, bias=True, w_bit=4, group_size=128)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): WQLinear_GEMM(in_features=2560, out_features=10240, bias=True, w_bit=4, group_size=128)\n",
       "            (fc2): WQLinear_GEMM(in_features=10240, out_features=2560, bias=True, w_bit=4, group_size=128)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:52:26.789624Z",
     "iopub.status.busy": "2024-04-08T11:52:26.789240Z",
     "iopub.status.idle": "2024-04-08T11:52:27.845147Z",
     "shell.execute_reply": "2024-04-08T11:52:27.844559Z",
     "shell.execute_reply.started": "2024-04-08T11:52:26.789597Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer_awq = AutoTokenizer.from_pretrained(quant_model_dir)\n",
    "model_awq = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:52:29.657516Z",
     "iopub.status.busy": "2024-04-08T11:52:29.657021Z",
     "iopub.status.idle": "2024-04-08T11:52:29.660860Z",
     "shell.execute_reply": "2024-04-08T11:52:29.660276Z",
     "shell.execute_reply.started": "2024-04-08T11:52:29.657491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(text,tokenizer,model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:52:55.768983Z",
     "iopub.status.busy": "2024-04-08T11:52:55.768613Z",
     "iopub.status.idle": "2024-04-08T11:53:11.150765Z",
     "shell.execute_reply": "2024-04-08T11:53:11.150208Z",
     "shell.execute_reply.started": "2024-04-08T11:52:55.768960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to\n",
      "M?m\n",
      "M\n",
      "Mm\n",
      "\n",
      "M\n",
      "Mm\n",
      "M\n",
      "\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "\n",
      "M\n",
      "\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "\n",
      "M\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\",tokenizer_awq,model_awq)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-08T11:53:52.776114Z",
     "iopub.status.busy": "2024-04-08T11:53:52.775736Z",
     "iopub.status.idle": "2024-04-08T11:53:56.560850Z",
     "shell.execute_reply": "2024-04-08T11:53:56.560075Z",
     "shell.execute_reply.started": "2024-04-08T11:53:52.776088Z"
    },
    "id": "Z0hAXYanCDW3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a the woman\n",
      "The the woman woman\n",
      "The the the the the the the\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\",tokenizer_awq,model_awq)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
